{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机森林方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 基于树的分类和回归方法 \n",
    "\n",
    "基于树的分类和回归方法;\n",
    "\n",
    "学习回归和分类中的损失函数."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 什么是决策树?\n",
    "**决策树(Decision Tree)**是由结点和有向边组成的树,其中结点包括内部结点和叶结点.内部结点表示一个特征或属性,\n",
    "叶结点表示一个类.\n",
    "\n",
    "**决策树算法**是一种常用的机器学习算法,在分类问题中,它通过样本中某一维属性\n",
    "的值将样本划分到不同的类别中.\n",
    "决策树算法是基于树结构进行决策的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 基于树的方法.\n",
    "\n",
    "随机森林方法用于预测时,一般会用到数棵树.\n",
    "\n",
    "在图论的理论中,树的集合,称为森林.所以该方法称为\"森林\".\n",
    "\n",
    "也就是说,为了作预测,随机森林考虑数棵树的预测(结果). \n",
    "\n",
    "显然,利用多棵__相同__的树做预测,是没有什么意义的, 因为它们都会给出**同样**的预测结果. \n",
    "所以,我们需要**不同**的树,这就是该方法称为\"随机森林\"的原因.\n",
    "\n",
    "基于树的方法可以用于分类,也可用于回归.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征空间(feature space/ predictor space):\n",
    "\n",
    "该方法利用一系列**直线**将预测空间分离为多个简单区域.\n",
    "\n",
    "具体地: \n",
    "\n",
    "首先, 将特征空间划分为两个区域,\n",
    "然后,再轮流看这两个更小的区域, 把它们分离为更小的区域, ...\n",
    "直到我们达到我们的某个可以停止的标准.\n",
    "\n",
    "这种把特征空间划分为更小的多个区域的方法本质上是**递归**的.\n",
    "\n",
    "为了对测试数据做预测, 我们找到该测试数据点落在特征空间的那个区域.\n",
    "\n",
    "在**分类问题**中,我们返回那一个区域中训练数据的目标值的**模式(mode)**,即,出现频率最高的那一个类别.\n",
    "\n",
    "在**回归问题**中,我们返回那一个区域中训练数据的目标(标签)的**平均值**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一个约束条件: \n",
    "\n",
    "当我们利用直线将特征空间分割为数个区域时,这些**直线必须与特征空间的坐标轴方向**对齐. \n",
    "\n",
    "由于这个约束条件, 我们可以将分割规则用一棵树表示. 所以该方法又称\"决策树算法\".\n",
    "\n",
    "在高维情况,这些直线变成平面, 因此,我们利用此方法得到的结果是: **将特征空间分割成为数个高维矩形**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 问题: 我们从哪里分割特征空间呢? \n",
    "\n",
    "基本思想: \n",
    "我们这样分割特征空间: **使得特征空间中这些区域上的标签值(outcomes)最大化地平均**.\n",
    "\n",
    "注意: 我们最终使用落在一个给定区域的标签(目标)的**模式(mode)或平均值**\n",
    "来作为我们对测试数据(未见过的数据)的预测结果.\n",
    "\n",
    "因此,我们可以通过**找到特征空间中最大平均化的区域**来尽量减少误差.\n",
    "\n",
    "每当我们做分割,我们考虑所有的特征(predictors,features)\n",
    "\n",
    "$x_1, ..., x_p$\n",
    "\n",
    "且,对于每一个特征,我们考虑所有可能的分割点.\n",
    "\n",
    "\n",
    "我们可以定义某种标准---损失函数,\n",
    "\n",
    "**我们选择(特征--分割点, axis--value)组合以使得最终特征空间的分割方式让损失函数的值最小**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数: loss function.\n",
    "\n",
    "在回归中: 通常选残差平方和(rss)作为损失函数.\n",
    "\n",
    "在分类中: 通常选Gini指数, 或交叉熵(cross-entropy)作为损失函数.\n",
    "\n",
    "它们的基本思想: 利用predictor-cut组合来分割特征空间,使得特征空间的每一区域尽量地均匀化(homogeneous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 随机森林预测\n",
    "如何集成多棵树的预测结果来做随机森林分类和回归\n",
    "\n",
    "学习随机森林方法引入的两种类型的随机性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 1. 数据中的随机性: 称为bagging.\n",
    "\n",
    "Bootstrap是一种re-sampling方法,它主要包括重复不停地从训练集中提取样本,并对该样本拟合出一个相应的模型.\n",
    "\n",
    "如果在我们的训练集中有$n$个观测结果,通过随机地选择这$n$个观测值,我们构建了一个bootstrap数据集. \n",
    "\n",
    "由于这里的采样实现方法是取代, 同样的观测可在这个bootstrap数据集中出现多次.\n",
    "\n",
    "我们可以将这个过程(?)实施多次, 那么每次我们将得到稍微不同的数据集.\n",
    "\n",
    "因此,在决策树算法中, bagging,的意思是: 提取多个bootsrap数据集,并拟合其中的每一个到一棵树.\n",
    "\n",
    "#### 2.第二种随机性: 如何分割特征空间.\n",
    "\n",
    "通常,利用决策树,当我们在特征空间做分割时,我们考虑每个特征--分割点(axis--value)组合.\n",
    "\n",
    "与之相反, 在随机森林中,每一次我们考虑一个分割. \n",
    "\n",
    "我们不看所有的特征,而是**随机地对特征采样**(提取出数量更少的特征), 当我们做特征空间分割时,我们只允许利用这些提取出来的特征.\n",
    "\n",
    "每当我们做特征空间分割,我们得到一个新的\"特征样本\". 这是一种非常有效的手段.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例.\n",
    "\n",
    "考虑一个具有1000个观测值的数据集(1000个样本). 我们有9个特征:$x_1, x_2, ..., x_9$.\n",
    "比如,我们想建立50棵树. 首先将数据随机化.\n",
    "\n",
    "我们首先从最初的数据中提取出50个bootstrap样本, 并对每个数据集都单独建立一棵树.\n",
    "\n",
    "然后, 我们一一地拟合这些树. 从第一棵树和第一次分割,我们首先决定使用哪一个特征.\n",
    "\n",
    "比如,当我们做第一次特征空间分割时, 如果我们被允许使用三个特征,(如$X_2, X_6, X_7)$. \n",
    "对于已知数据和这三个特征,我们尽力做最好的分割.  \n",
    "\n",
    "然后,我们对该树做第二次分割.\n",
    "这次,我们可能被允许使用的三个特征为$X_1,X_5,X_7$, 我们同样地尽力做最好的分割.\n",
    "\n",
    "我们继续,直到我们拟合了第一棵树,也就是说,直到我们达到某个停止分割的标准.\n",
    "\n",
    "然后,我们对森林中的其他所有的树实施同样的操作.\n",
    "\n",
    "为了利用随机森林做预测, 对每一棵树,我们分别识别测试数据所在的那些特征空间区域.\n",
    "\n",
    "基于此, 我们接下来,对于每一棵树我们分别做一次预测.\n",
    "并将每个单独的树的预测结果结合起来, 形成森林的预测结果.\n",
    "\n",
    "这就是随机森林的工作原理."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 练习\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
