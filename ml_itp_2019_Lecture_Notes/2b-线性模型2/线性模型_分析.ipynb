{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 一般线性回归\n",
    "为了对比, 回顾一下前面学过的kNN回归算法的结果:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块:\n",
    "import mglearn \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.neighbors import KNeighborsRegressor \n",
    "from sklearn.linear_model import LinearRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x17329b71198>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#生成数据集: 只有一个特征的数据集\n",
    "X, y = mglearn.datasets.make_wave(n_samples=40)  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "\n",
    "#如果数据仅有一个特征,利用array.reshape(-1, 1)改变数组形状;\n",
    "#如果数据中仅包含一个样本,则可利用array.reshape(1, -1)改变数组形状.\n",
    "line = np.linspace(-3,3, 1000).reshape(-1,1)\n",
    "n_neighbors = 7 # 可改变k值:1,5,10\n",
    "\n",
    "# 训练回归模型\n",
    "reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "reg.fit(X_train,y_train)\n",
    "\n",
    "# 作图: 包括回归曲线, 训练集,测试集 共三行代码\n",
    "# ==Your code\n",
    "\n",
    "# 图形标识\n",
    "plt.title(\"{} neighbor(s)\\n train score:{:.2f} test score: {:.2f}\".format(n_neighbors, reg.score(X_train,y_train),                                                                        reg.score(X_test,y_test)))\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.legend([\"Predictions\", \"Training data/target\", \"Test data/target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 简单线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 对于只有一个特征的数据集\n",
    "线性回归模型\n",
    "\n",
    "$\\hat y = \\beta_1 * x_1 + \\beta_0$,\n",
    "\n",
    "$\\beta_1$是斜率, $\\beta_0$是y轴上的截距.\n",
    "\n",
    "$\\hat y$称为response, 是模型的\\____.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### (2) 对于只有$p+1$个特征的数据集\n",
    "线性回归模型\n",
    "\n",
    "$\\hat y = \\beta_0* x_0+ ...+ \\beta_p * x_n$ (总是设置 $x_0 = 1$)\n",
    "\n",
    "我们也可把$\\hat y$看作是输入特征$x_0,x_1,...,x_n$的带权重的求和.($n$ : 特征的数目.)\n",
    "\n",
    "我们可将系数记为一个矢量$\\bf \\beta$\n",
    "$${\\bf \\beta} = \\left(\n",
    "\\begin{matrix}\n",
    " \\beta_0    \\\\\n",
    " \\beta_1 \\\\\n",
    " \\vdots \\\\\n",
    " \\beta_n \\\\\n",
    "\\end{matrix} \n",
    "\\right),\n",
    "$$\n",
    "其中,${\\bf\\beta}_0, {\\bf\\beta}_1,...,{\\bf\\beta}_n$是模型的参数.\n",
    "\n",
    "\n",
    "可将**一个样本**的特征记为一个矢量$\\bf x$ \n",
    "$${\\bf x} = \\left(\n",
    "\\begin{matrix}\n",
    " 1  \\\\\n",
    " x_1 \\\\\n",
    " \\vdots \\\\\n",
    " x_n \\\\\n",
    "\\end{matrix} \n",
    "\\right),\n",
    "$$\n",
    "其中,$x_1$表示第1个特征, ...,$n$表示第$n$个特征.\n",
    "\n",
    "所以, 我们可以写出线性回归模型的矢量形式:\n",
    "$$\\hat y = {\\bf\\beta}^T \\cdot {\\bf x},$$\n",
    "\n",
    "其中,${\\bf\\beta}^T$是$\\bf\\beta$的转置 (所以是一个**行矢量**), $\\hat y$ 是 ${\\bf\\beta}^T$ 和 ${\\bf x}$的点积."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 代价函数(cost function) \n",
    "又称损失函数(loss funtion).\n",
    "\n",
    "实践中,往往计算均方差损失函数(MSE lost function), 从而获得参数${\\bf\\beta}$的值(训练出模型).  (对比: 前面学过RMSE, 但我们计算MSE,而不是RMSE.)\n",
    "\n",
    "$$\\text{MSE}({\\bf\\beta}) = \\frac{1}{m}\\sum_{i=1}^m({\\bf\\beta}^T \\cdot {\\bf x}^{(i)} -y^{(i)})^2.$$\n",
    "\n",
    "解出上面这个方程,就可以得到$\\bf\\beta$的值,也就是**线性模型**. 其解为:\n",
    "\n",
    "$$\\hat{\\bf \\beta} = ({\\bf X}^T\\cdot {\\bf X})^{-1}\\cdot {\\bf X}^T \\cdot {\\bf y},$$\n",
    "\n",
    "$\\hat{\\bf \\beta}$就是使得损失函数最小的参数组成的矢量;\n",
    "\n",
    "$\\bf X$是$m\\times n$矩阵, $\\bf\\beta$是$n\\times 1$矩阵(矢量);\n",
    "\n",
    "${\\bf y}$是目标值$y^{(1), ..., y^{(m)}}$构成的矢量, $n\\times 1$矩阵.\n",
    "\n",
    "我们称这个方程为**正则方程** (Normal Equation). \n",
    "\n",
    "注意:用正则方程来计算$\\bf\\beta$的时间复杂度为$O(n^3)$, 当特征数目$n$很大时,不要用这种方法来求$\\bf\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_regression.py  \n",
    "\n",
    "# 导入线性回归模型\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#生成数据集: 只有一个特征的数据集\n",
    "X, y = mglearn.datasets.make_wave(n_samples=40)  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)  \n",
    "# print(X.shape)\n",
    "\n",
    "# 训练出线性回归模型\n",
    "#==Your code\n",
    "\n",
    "\n",
    "# 计算并打印斜率和截距\n",
    "# w are stored in the coef_ attribute,  \n",
    "# b is stored in the intercept_ attribute. \n",
    "# ==Your code\n",
    "\n",
    "# 分别计算训练集和测试集的拟合优度, 保留三位小数 \n",
    "#==Your code\n",
    "\n",
    "# 作图:\n",
    "# 对回归方程作图; 对训练集数据作图; 对测试集数据作图\n",
    "# ==Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据的很多细节都被丢掉了. **我们的目标(Target标签)是特征的( )** 是一个很强的假设,也就是说,是过于( )的模型! \n",
    "\n",
    "    1. coef_是Numpy数组；　intercept_是一个浮点型数.\n",
    "    2. 训练集和测试集的准确度都不高.这说明我们的方法是 ()拟合的，而不是()拟合的.\n",
    "    3. 但是对于具有更多特征的数据集，线性模型是很有用的！这种情况下，更多的情况是过拟合出现，而非欠拟合.(例)\n",
    "\n",
    "当你的特征数目($p+1$)大于你的数据点的数目(样本个数)时,目标$y$往往可以用线性模型很好地预测出来. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面就是一个例子.这个例子中,特征数目为(),样本数目(). 前者( )后者."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr.coef_:[-1.28951445e+02 -1.34428510e+00 -8.67318455e+01  7.77632358e+00\n",
      " -3.99171323e+01  2.70293830e+01  5.47208851e+01 -6.56642421e+01\n",
      "  5.09798804e+01  2.65026504e+01 -1.30928694e+01  4.74536524e+01\n",
      "  6.03409502e+01  5.10356536e+01  1.78502944e+03  2.28227572e+02\n",
      "  4.20670688e+02 -8.17911638e+01  2.68870969e+02 -1.03756878e+02\n",
      " -3.71719154e+02 -4.05628354e+02  3.86397260e+02  1.60061974e+00\n",
      "  2.40250959e+01  6.01420724e+01 -5.58474352e+00 -1.21070403e+01\n",
      " -1.27840747e+01 -2.75737758e+01 -3.09926850e+00 -3.67752456e-01\n",
      " -4.97206841e+00 -1.83700135e+01  2.20802645e+01 -8.71607064e+00\n",
      "  2.01513767e+01 -2.63241394e+01  2.12213309e+01  2.75196442e+00\n",
      "  1.24451343e+01  4.12662428e+01  1.59151010e+01  5.68862279e+01\n",
      " -1.71528356e+01  1.68930772e+01  4.43661965e+00  1.96345586e+01\n",
      "  3.33113995e+00  7.77632358e+00 -1.19634667e+01 -3.62913234e+01\n",
      "  3.17167393e+00  3.67202215e+01 -3.69170261e+00 -8.75746266e+00\n",
      " -9.76048562e+00  9.49754359e+00 -2.02144606e+01  1.95083142e+01\n",
      "  2.32452089e+01 -2.32090567e+01  1.09897457e+02 -5.65403462e+00\n",
      "  1.82206197e+01 -2.40725372e+01  3.25570869e+00  2.28703328e+01\n",
      "  2.69598229e+01 -3.65331029e+01  2.95197946e+01 -2.42789751e+01\n",
      " -3.85761634e+01 -7.47720856e+00  1.08916581e+01 -7.46418921e+01\n",
      "  4.70679858e+00 -7.06500937e+00  2.92461833e+01 -2.63284366e+01\n",
      " -2.18077945e+00 -3.83661698e+01 -2.28775602e+01  6.78604641e+01\n",
      " -2.03876378e+01 -2.62335193e+01 -5.62511005e+00 -3.29828098e+01\n",
      "  4.86983530e+01 -7.64545861e+01  9.34630920e+01 -2.61806424e+01\n",
      " -8.23391783e+00 -1.10643585e+01 -1.20961612e+01  2.20278818e+01\n",
      " -1.96245095e+01 -2.80329087e+01  1.53095029e+00  1.95557966e+01\n",
      "  4.93387371e+00 -9.69755206e+00 -2.19874572e+01 -1.06940800e+01]\n",
      "lr.intercept_:-17.192334944481043\n",
      "Traing set score: 0.93\n",
      "Test set score:0.23\n"
     ]
    }
   ],
   "source": [
    "#linear_regression2.py  \n",
    "\n",
    "#生成数据集\n",
    "X, y = mglearn.datasets.load_extended_boston()    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=40)  \n",
    "#查看训练集的形状\n",
    "#==Your code\n",
    "\n",
    "# ( )\n",
    "# your code\n",
    "  \n",
    "#计算并打印( )\n",
    "# w are stored in the coef_ attribute,  \n",
    "# b is stored in the intercept_ attribute.  \n",
    "print(\"lr.coef_:{}\".format(lr.coef_))  \n",
    "print(\"lr.intercept_:{}\".format(lr.intercept_))  \n",
    "\n",
    "# 分别计算训练集和测试集的拟合优度, 保留三位小数 \n",
    "print(\"Traing set score: {:.2f}\".format(lr.score(X_train,y_train)))  \n",
    "print(\"Test set score:{:.2f}\".format(lr.score(X_test,y_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析:\n",
    "1. 对于训练集，我们预测得( ) ，然而对于测试集上预测结果却( ).$R^2= ( ) $,比较() ,说明回归模型 ( )  ！\n",
    "2. 上述差别说明：出现了 ( )！ 可以猜测: 对于特征数目很多的数据集(常称为**高维度数据集**),线性模型变得更有用,同时更容易出现( ).\n",
    "3. 我们需要一个能让我们控制复杂度的模型. 其中一个常见的选择是:岭回归.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 岭回归(Ridge regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge回归也是一种线性回归. 这个方法在OLS基础上，加入了额外的约束条件.\n",
    "\n",
    "另外，我们也想让系数的绝对值尽可能的( )，即w的所有元素都应该接近0．\n",
    "\n",
    "要求每个特征尽可能少地影响输出结果，也就是说，斜率要尽可能地( ),但是预测要尽可能准确！\n",
    "\n",
    "加这个约束条件是**正则化**（regularization）的一个例子. \n",
    "\n",
    "正则化的含义就是：约束一个模型以避免过拟合.\n",
    "\n",
    "岭回归用的正则化方法是L2正则化. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "岭回归的cost function (代价函数/损失函数)$J(\\beta)$为:\n",
    "\n",
    "$$J(\\beta) = \\text{MSE}(\\beta) + \\alpha \\frac{1}{2}\\sum_{i=1}^p \\beta_i^2$$ \n",
    "\n",
    "(注意: $\\beta_0$没有被正则化.)\n",
    "\n",
    "求解$\\bf \\beta$的常用方法:\n",
    "\n",
    "直接求解法. 公式表示为\n",
    "$$\\hat{\\bf \\beta} = ({\\bf X}^T\\cdot {\\bf X} + \\alpha {\\bf I})^{-1}\\cdot {\\bf X}^T \\cdot {\\bf y} $$\n",
    " \n",
    "梯度下降法(Gradient Descent). (类比前面学过的最小二乘法)\n",
    "\n",
    "公式表达为:\n",
    "$$\\nabla_{\\beta}\\text{MSE}({\\bf\\beta}) \n",
    "= \\left(\n",
    "\\begin{matrix}\n",
    " \\nabla_{\\beta_0}\\text{MSE}({\\bf\\beta})  \\\\\n",
    " \\nabla_{\\beta_1}\\text{MSE}({\\bf\\beta}) \\\\\n",
    " \\vdots \\\\\n",
    " \\nabla_{\\beta_n}\\text{MSE}({\\bf\\beta})\\\\\n",
    "\\end{matrix} \n",
    "\\right)\n",
    "= \\frac{2}{m}{\\bf X}^T \\cdot ({\\bf X}{\\bf\\beta} - {\\bf y});\n",
    "$$\n",
    "\n",
    "\n",
    "$${\\bf\\beta}^{(\\text{next step})} = {\\bf\\beta} - \\eta\\nabla_{\\beta}\\text{MSE}({\\bf\\beta}).$$\n",
    "\n",
    "\n",
    "岭回归算法已经在\n",
    "    sklearn.linear_mode.Ridge\n",
    "中实现了."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 104)\n",
      "ridge.coef_:[-1.70295812e+00 -1.21646220e+00 -2.26391270e+00  8.30845896e-01\n",
      " -8.58331367e-02  8.01116071e+00 -5.65999560e-03 -4.69520099e+00\n",
      "  3.64886980e+00 -1.76903379e+00 -1.84601461e+00  2.32007300e+00\n",
      " -2.80633098e+00 -1.00796122e+00  5.43017764e-03 -1.05926024e+00\n",
      "  1.58394405e+00 -1.66367827e+00 -1.33734211e+00 -1.24701369e+00\n",
      " -3.45676068e-01 -1.97789722e+00 -1.71426435e+00 -1.55777662e+00\n",
      " -1.22046160e+00 -1.18685106e+00  1.86974330e+00 -1.84090506e+00\n",
      "  2.15362590e+00  3.33286814e-01  3.44159376e+00 -1.96103914e+00\n",
      " -4.39026619e-01 -1.86641840e-01  3.53642680e-01  6.52130195e-01\n",
      " -1.00510187e+00 -1.77202481e+00  3.42271888e+00  1.40820125e+00\n",
      "  8.83430865e-01 -3.91620276e+00  2.18270833e+00 -3.72297245e+00\n",
      "  1.10970075e+00  2.54657815e+00 -1.16437597e+00 -5.84765670e-01\n",
      " -1.92913457e+00  8.30845896e-01 -4.88404275e+00 -3.65276071e+00\n",
      "  1.33691241e+00 -1.17583545e+00  2.82947313e+00  3.21863173e+00\n",
      "  3.94714722e-01  1.80083969e+00 -2.30349790e+00 -1.68219494e+00\n",
      " -3.55230706e+00 -2.18599021e+00 -9.27982628e-01 -2.53092628e+00\n",
      " -1.58471861e+00 -2.87271300e+00  1.62300691e+00 -1.90732301e+00\n",
      "  1.49389562e+01 -3.87316528e-01  1.24464700e+00 -6.64540858e+00\n",
      " -7.52121608e+00 -4.29542567e+00  1.07727555e+01 -6.29926886e+00\n",
      "  1.32751219e+00 -1.84322821e+00  3.61675455e+00  4.16241023e-01\n",
      " -1.69159673e+00 -1.97566230e+00 -3.25849616e+00  1.11350329e+00\n",
      " -1.60578810e+00 -2.75642376e+00 -7.75620554e-01 -3.82545173e+00\n",
      "  3.69223888e-01  7.20424872e-01  7.71237641e-01  2.50362234e+00\n",
      "  2.60244298e+00 -6.29040903e+00  1.51485823e+00  1.18394049e+00\n",
      " -1.15724635e+00 -6.39479783e+00  6.52519686e-01 -1.79273639e+00\n",
      " -1.73340672e+00 -1.03519815e+00 -5.83840387e+00  6.39666192e+00]\n",
      "ridge.intercept_:21.90287999498934\n",
      "Traing set score: 0.834\n",
      "Test set score:0.860\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge  \n",
    "# 生成数据  \n",
    "X, y = mglearn.datasets.load_extended_boston()   \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=40)  \n",
    "\n",
    "# __训练岭回归模型__\n",
    "# ==Your code \n",
    "  \n",
    "\n",
    "# __计算系数和截距__\n",
    "# w are stored in the coef_ attribute,  \n",
    "# b is stored in the intercept_ attribute.   \n",
    "print(\"ridge.coef_:{}\".format(ridge.coef_))  \n",
    "print(\"ridge.intercept_:{}\".format(ridge.intercept_))  \n",
    "  \n",
    "# __分别计算训练集和测试集的拟合优度, 保留三位小数__\n",
    "print(\"Traing set score: {:.3f}\".format(ridge.score(X_train,y_train)))  \n",
    "print(\"Test set score:{:.3f}\".format(ridge.score(X_test,y_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析:\n",
    "\n",
    "1. 使用了岭回归以后,训练集的$R^2$值变()了,但测试集的$R^2$值更()了! 符合预期吗?. () \n",
    "2. 用一般线性回归,我们得到过拟合的模型.\n",
    "3. 复杂性更低的模型在训练集上表现得更() ,但更具有( ). \n",
    "4. 普遍性是我们需求的,所以,我们应该选择(岭回归 or 一般线性回归)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 岭回归 设置alpha=10  \n",
    "X, y = mglearn.datasets.load_extended_boston()   \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=40)  \n",
    "# 训练模型\n",
    "#== YOur code\n",
    "\n",
    "# w are stored in the coef_ attribute,  \n",
    "# b is stored in the intercept_ attribute.  \n",
    "print(\"ridge.coef_:{}\".format(ridge10.coef_))  \n",
    "print(\"ridge.intercept_:{}\".format(ridge10.intercept_))  \n",
    "\n",
    "# 分别计算训练集和测试集的拟合优度, 保留三位小数\n",
    "print(\"Traing set score: {:.3f}\".format(ridge.score(X_train,y_train)))  \n",
    "print(\"Test set score:{:.3f}\".format(ridge.score(X_test,y_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 岭回归: 设置 alpha = 0.1 设置 alpha适当地小 模型更具普遍性)\n",
    "  \n",
    "X, y = mglearn.datasets.load_extended_boston()   \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=40)  \n",
    "\n",
    "# 训练模型  \n",
    "# your code\n",
    "print(X.shape)  \n",
    "\n",
    "# w are stored in the coef_ attribute,  \n",
    "# b is stored in the intercept_ attribute.  \n",
    "print(\"ridge.coef_:{}\".format(ridge.coef_))  \n",
    "print(\"ridge.intercept_:{}\".format(ridge.intercept_))  \n",
    "\n",
    "# 分别计算训练集和测试集的拟合优度, 保留三位小数\n",
    "print(\"Traing set score: {:.3f}\".format(ridge.score(X_train,y_train)))  \n",
    "print(\"Test set score:{:.3f}\".format(ridge.score(X_test,y_test))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 续\n",
    "# 系数值与alpha的依赖关系\n",
    "plt.plot(ridge.coef_, \"s\", label=\"Ridge alpha=1\")\n",
    "plt.plot(ridge10.coef_, \"^\", label=\"Ridge alpha=10\")\n",
    "plt.plot(ridge01.coef_, \"v\", label=\"Ridge alpha=0.1\")\n",
    "print(X.shape) \n",
    "\n",
    "# LR\n",
    "plt.plot(lr.coef_, \"o\", label=\"LinearRegression\")\n",
    "plt.xlabel(\"coeficient index\")\n",
    "plt.ylabel(\"Coefficient value\")\n",
    "plt.hlines(0,0,len(lr.coef_))\n",
    "\n",
    "plt.ylim(-25, 25)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析:\n",
    "1. x轴: 模型的系数的() . 比如coef.index=0 对应第( )特征, coef.index=1对应第( )特征, ...\n",
    "\n",
    "2. y轴: 模型的( ).\n",
    "\n",
    "3. 岭回归: alpha=10对应的系数值取值大约在( )!\n",
    "\n",
    "4. 岭回归: alpha=1, alpha=0.1对应的系数值取值的范围都要( ).\n",
    "\n",
    "5. 无正则化的线性回归方法得到的系数值,范围更大还是更小? ( ) 是我们想要的结果吗? ( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lasso回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso回归(Least Absolute Shrinkage and Selection Operator regression),\n",
    "\n",
    "另一种正规化线性回归方法. \n",
    "\n",
    "与岭回归一样,该法也是添加一个**正则项**到损失函数(cost function),使系数尽量接近0.\n",
    "\n",
    "在Lasso回归模型中，我们用的是L1正规化: \n",
    "\n",
    "使用L1正则化后,一些最不重要的系数**精确地**等于0 (有些特征直接被忽略掉了).\n",
    "\n",
    "这可以视为自动特征选择的的一种形式.\n",
    "\n",
    "有些特征被( ),使得我们构建了一个更易解释的模型(稀疏模型),\n",
    "\n",
    "能揭示数据最重要的特征.\n",
    "\n",
    "\n",
    "Lasso回归的损失函数$J(\\beta)$为:\n",
    "\n",
    "$$J(\\beta) = MSE(\\beta) + \\alpha \\sum_{i=1}^p |\\beta_i|$$ \n",
    "\n",
    "(注意: $\\beta_0$没有被正则化.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso_regression.py  \n",
    "from sklearn.linear_model import Lasso  \n",
    "\n",
    "# 生成模型  \n",
    "X, y = mglearn.datasets.load_extended_boston()   \n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=40)  \n",
    "\n",
    "# 训练Lasso回归模型\n",
    "# Your code\n",
    "\n",
    "print(\"lasso.coef_:{}\".format(lasso.coef_))  \n",
    "print(\"lasso.intercept_:{}\".format(lasso.intercept_))  \n",
    "  \n",
    "print(\"Traing set score: {:.2f}\".format(lasso.score(X_train,y_train)))  \n",
    "print(\"Test set score:{:.2f}\".format(lasso.score(X_test,y_test)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析:\n",
    "    1. Lasso回归对训练集和测试集的R^2分别为 (),() . 预测都做得(好,差). 说明出现了(欠拟合,还是过拟合)\n",
    "    2. 共有( )特征,只用到了( )个特征. \n",
    "    3. Lasso()有正规化参数alpha. 其默认值alpha=1.0. alpha的值描述将系数推向０的力度. \n",
    "\n",
    "为了减少欠拟合的影响，我们增加另一个参数max_iter之值. mat_iter的值为运行时迭代的次数之最大值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso001_regression.py   \n",
    "X, y = mglearn.datasets.load_extended_boston()  \n",
    "  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=40)  \n",
    "lasso01 = Lasso(alpha=0.01,max_iter=10000).fit(X_train, y_train)  \n",
    "  \n",
    "print(\"lasso.coef_:{}\".format(lasso01.coef_))  \n",
    "print(\"lasso.intercept_:{}\".format(lasso01.intercept_))  \n",
    "print(\"No.of features used:{}\".format(np.sum(lasso001.coef_!=0)))  \n",
    "\n",
    "print(\"Traing set score: {:.2f}\".format(lasso01.score(X_train,y_train)))  \n",
    "print(\"Test set score:{:.2f}\".format(lasso01.score(X_test,y_test)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析:\n",
    "    1. 更(高,低)的alpha值可使我们拟合出更复杂的模型,从而在训练集和测试集上都有(更好,更差)的预测性能.\n",
    "    2. 此例中,alpha=0.01, max_iter=100000. 相对默认值,训练集和测试集的拟合优度都有没有提升.(有,没有 )\n",
    "    3. 相比岭回归,这个Lasso回归模型的拟合优度更高,而所用的特征数只使105个特征中的(  )个.所以Lasso回归所得的模型比岭回归模型更(简单, 复杂),更易解释.\n",
    "\n",
    "alpha过小,显然会导致( )现象!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso0001_regression.py \n",
    "# 训练Lasso回归模型,lasso00001,设置alpha=0.0001, max_iter=10000\n",
    "  \n",
    "X, y = mglearn.datasets.load_extended_boston()  \n",
    "  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=40)  \n",
    "# 训练模型\n",
    "# your code\n",
    "  \n",
    "print(\"lasso.coef_:{}\".format(lasso00001.coef_))  \n",
    "print(\"lasso.intercept_:{}\".format(lasso00001.intercept_))  \n",
    "print(\"No.of features used:{}\".format(np.sum(lasso00001.coef_!=0)))  \n",
    "\n",
    "print(\"Traing set score: {:.2f}\".format(lasso00001.score(X_train,y_train)))  \n",
    "print(\"Test set score:{:.2f}\".format(lasso00001.score(X_test,y_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 续: 作出不同模型的系数的图\n",
    "# 系数值与Lasso alpha的依赖关系\n",
    "plt.plot(lasso.coef_, \"s\", label=\"Lass alpha=1\")\n",
    "plt.plot(lasso001.coef_, \"^\", label=\"Lasso alpha=0.01\")\n",
    "plt.plot(lasso00001.coef_, \"v\", label=\"Lasso alpha=0.0001\")\n",
    "\n",
    "plt.plot(ridge01.coef_, \"o\", label=\"Ridge alpha=0.1\") \n",
    "plt.xlabel(\"coeficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.ylim(-25, 25)\n",
    "plt.legend(ncol=2, loc=(0,1.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对系数的特征作分析:\n",
    "    1. 对alpha=1, ()\n",
    "    \n",
    "    2. 减小alpha到0.01, ( )\n",
    "    \n",
    "    3. 减小alpha到0.0001, ( )\n",
    "    \n",
    "   \n",
    "    \n",
    "    4. 岭回归和Lasso回归的预测能力对比: ( )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 线性分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性模型也可用于分类问题. 预测值为如下公式：\n",
    "\n",
    "$$\\hat y = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n $$\n",
    "\n",
    "这公式与一般线性回归非常相似.\n",
    "\n",
    "唯一不同的是,我们添加了线性分类模型的阈值0. \n",
    "\n",
    "线性分类模型的规则如下：\n",
    " \n",
    "如果 $\\hat y < 0$, 预测其类为　-1;\n",
    "\n",
    "如果 $\\hat y > 0$, 预测其类为　1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对线性回归模型,输出是特征的线性函数:\n",
    "\n",
    "一条直线,一个平面,或者一个超平面(高维空间中).\n",
    "\n",
    "|特征数 | 预测值$\\hat y$作为特征x的函数|\n",
    "|--------|-------------- |\n",
    "|1   | 2维空间中的一条直线 |\n",
    "|2   | 3维空间中的一个平面 |\n",
    "|3   | 4维空间中的一个超平面 |\n",
    "|4   | 5维空间中的一个超平面\n",
    "|5   | ... |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二元分类器就是用直线，平面或超平面将两类分开的分类器.\n",
    "\n",
    "这里的直线,平面,或超平面就是分类器的**决策边界**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前存在很多机器学习算法来构造线性分类器. \n",
    "\n",
    "它们的不同点只有如下两点:\n",
    "\n",
    "1. 判定斜率和截距拟合数据的好坏程度的方法. \n",
    "\n",
    "(数据拟合得很好的标准)\n",
    "\n",
    "2. 是否用正则化方法,用了什么正则化方法."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最常见的两种线性分类器算法是: \n",
    "\n",
    "逻辑回归(logistic regression)和\n",
    "\n",
    "支持向量机(linear support vector machines, linear SVMs).\n",
    "\n",
    "它们已经分别在\n",
    "    \n",
    "    linear_model.LogisticRegression\n",
    "    svm.LinearSVC\n",
    "\n",
    "中实现了. SVC代表support vector classifier(支持向量分类器)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.svm import LinearSVC  \n",
    "import mglearn\n",
    "\n",
    "X,y = mglearn.datasets.make_forge()  \n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,3))  \n",
    "\n",
    "for model, ax in zip([LinearSVC(), LogisticRegression()],axes):  \n",
    "    clf = model.fit(X,y)  \n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5, ax=ax, alpha=.7)  \n",
    "    mglearn.discrete_scatter(X[:, 0], X[:,1], y, ax=ax)  \n",
    "    ax.set_title(\"{}\".format(clf.__class__.__name__))  \n",
    "    ax.set_xlabel(\"X0\")  \n",
    "    ax.set_ylabel(\"X1\")  \n",
    "axes[0].legend()  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 线性支持向量机分类器 \n",
    "  \n",
    "X,y = mglearn.datasets.make_forge()   \n",
    "clf = LinearSVC().fit(X,y)  \n",
    "\n",
    "mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5, alpha=.7)  \n",
    "mglearn.discrete_scatter(X[:, 0], X[:,1], y)  \n",
    "plt.title(\"{}\".format(\"LinearSVC\"))  \n",
    "plt.xlabel(\"X0\")  \n",
    "plt.ylabel(\"X1\")  \n",
    "plt.legend()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 续:逻辑回归分类器  \n",
    "X,y = mglearn.datasets.make_forge()   \n",
    "clf = LogisticRegression().fit(X,y)  \n",
    "\n",
    "mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5, alpha=.7)  \n",
    "mglearn.discrete_scatter(X[:, 0], X[:,1], y)  \n",
    "plt.title(\"{}\".format(\"LogisticRegression\"))  \n",
    "plt.xlabel(\"X0\")  \n",
    "plt.ylabel(\"X1\")  \n",
    "plt.legend()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析: \n",
    "\n",
    "1. x轴是数据集的第一个特征,y轴是数据集的第二个特征\n",
    "\n",
    "2. 我们分别用线性支持向量机分类器和逻辑回归分类器计算出了决策边界. \n",
    "\n",
    "它是一条直线.\n",
    "\n",
    "3. 任何新数据如果落在上方,则归为1类; 如果落在下方,则归为0类.\n",
    "\n",
    "4. 这两种分类器算出的决策边界相似. 注意: 都有两个数据点分类有误!\n",
    "\n",
    "5. 两种分类算法都使用L2正则化方法."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在LogisticRegression和LinearSVC中,控制正则化的参数为C. \n",
    "\n",
    "C值越大,意味着更少的正则化."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性支持向量机分类器: C的取值,对决策边界的影响 \n",
    "X,y = mglearn.datasets.make_forge()   \n",
    "clf = LinearSVC(C=0.01).fit(X,y)  \n",
    "\n",
    "mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5, alpha=.7)  \n",
    "mglearn.discrete_scatter(X[:, 0], X[:,1], y)  \n",
    "plt.title(\"{}\".format(\"LinearSVC\"))  \n",
    "plt.xlabel(\"X0\")  \n",
    "plt.ylabel(\"X1\")  \n",
    "plt.legend()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性支持向量机分类器: C=100,对决策边界的影响(2)   \n",
    "  \n",
    "X,y = mglearn.datasets.make_forge()   \n",
    "clf = LinearSVC(C=100).fit(X,y)  \n",
    "\n",
    "mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5, alpha=.7)  \n",
    "mglearn.discrete_scatter(X[:, 0], X[:,1], y)  \n",
    "plt.title(\"{}\".format(\"LinearSVC\"))  \n",
    "plt.xlabel(\"X0\")  \n",
    "plt.ylabel(\"X1\")  \n",
    "plt.legend()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析:\n",
    "    1. C越小,边界越( ) (对训练集的拟合优度可能小一些: 可能存在相对错误的分类. 用了正则化方法!是我们期待的结果)\n",
    "    2. C越大,边界越( ).(对训练集的拟合优度可能非常高:出现(过拟合, 欠拟合).\n",
    "\n",
    "与线性回归一样,对低维数据集来说, 线性分类器中的\"线性\"太苛刻了.当数据集的特征较多时,线性模型用来做分类器就非常强大!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练逻辑回归模型lorreg: 以含有多个特征的数据集为例: 默认C=1\n",
    "from sklearn.datasets import  load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train,y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target,\n",
    "                 random_state=42 )\n",
    "# 训练模型\n",
    "# your code\n",
    "\n",
    "print(\"Traing set score: {:.3f}\".format(logreg.score(X_train,y_train)))  \n",
    "print(\"Test set score:{:.3f}\".format(logreg.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析: \n",
    "1. 默认值C=1 给出了很好的预测结果. 对训练集和测试集都很高.\n",
    "2. 由于两者很接近,说明出现了:(欠拟合,过拟合)! 所以C 可以选(更大,更小)的值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练逻辑回归模型lorreg100\n",
    "# 以含有多个特征的数据集为例: 选C=100.即更复杂的模型.\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train,y_test = train_test_split(cancer.data, cancer.target, \n",
    "                                                   stratify=cancer.target,\n",
    "                 random_state=42 )\n",
    "# 训练模型\n",
    "#==Your code\n",
    "\n",
    "print(X_train.shape)\n",
    "print(\"Traing set score: {:.3f}\".format(logreg100.score(X_train,y_train)))  \n",
    "print(\"Test set score:{:.3f}\".format(logreg100.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析:\n",
    "   1. C=100, 模型更复杂了.(正则化更少)\n",
    "   2. 训练集和测试集的拟合优度都有没有提高 分别为(),()!\n",
    "   \n",
    "正则化更多(C取值更小)时,模型会怎么样?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练逻辑回归模型lorreg001\n",
    "# 以含有多个特征的数据集为例: 选C=0.01.即更多正则化的模型.\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train,y_test = train_test_split(cancer.data, cancer.target, \n",
    "                                                   stratify=cancer.target,\n",
    "                 random_state=42 )\n",
    "\n",
    "# 训练模型\n",
    "#==Your code\n",
    "\n",
    "print(X_train.shape)\n",
    "print(\"Traing set score: {:.3f}\".format(logreg001.score(X_train,y_train)))  \n",
    "print(\"Test set score:{:.3f}\".format(logreg001.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析:\n",
    "    1. C=0.01时,训练集和测试集的拟合优度(升高,降低)了!\n",
    "    2. 当C过小时,得到一个(过拟合, 欠拟合)的模型(underfit model or overfit model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面考察C值的不同,对线性模型的系数值的影响."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 续: 作出不同模型的系数的图\n",
    "# 系数值与C的依赖关系\n",
    "plt.plot(logreg.coef_.T, \"s\", label=\"C=1\")\n",
    "plt.plot(logreg100.coef_.T, \"^\", label=\"C=100\")\n",
    "plt.plot(lasso001.coef_.T, \"v\", label=\"C=0.001\")\n",
    "print(logreg.coef_.shape)\n",
    "print(logreg.coef_.T.shape)\n",
    "print(cancer.data.shape)\n",
    "\n",
    "plt.xlabel(\"coeficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.ylim(-5, 5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析:\n",
    "    \n",
    "    logreg.coef_: 二维数组, 里面只有一个元素,该元素为一个一维数组.\n",
    "    logreg.coef_.T: 形状为(30,1)的2维数组.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们想要更简单的模型,可以选用L1正则化方法. 可设置penalty=\"l1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以含有多个特征的数据集为例: 选不同的C,附加参数penalty=\"l1\".\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train,y_test = train_test_split(cancer.data, cancer.target, \n",
    "                                                   stratify=cancer.target,\n",
    "                 random_state=42 )\n",
    "lr_l1 = LogisticRegression(C=0.01,penalty=\"l1\").fit(X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(\"Traing set score: {:.3f}\".format(lr_l1.score(X_train,y_train)))  \n",
    "print(\"Test set score:{:.3f}\".format(lr_l1.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以含有多个特征的数据集为例: 选不同的C,附加参数penalty=\"l1\".\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train,y_test = train_test_split(cancer.data, cancer.target, \n",
    "                                                   stratify=cancer.target,\n",
    "                 random_state=42 )\n",
    "lr_l1 = LogisticRegression(C=1,penalty=\"l1\").fit(X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(\"Traing set score: {:.3f}\".format(lr_l1.score(X_train,y_train)))  \n",
    "print(\"Test set score:{:.3f}\".format(lr_l1.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以含有多个特征的数据集为例: 选不同的C,附加参数penalty=\"l1\".\n",
    "from sklearn.datasets import  load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train,y_test = train_test_split(cancer.data, cancer.target, \n",
    "                            stratify=cancer.target, random_state=42 )\n",
    "lr_l1 = LogisticRegression(C=100,penalty=\"l1\").fit(X_train, y_train)\n",
    "print(X_train.shape)\n",
    "print(\"Traing set score: {:.3f}\".format(lr_l1.score(X_train,y_train)))  \n",
    "print(\"Test set score:{:.3f}\".format(lr_l1.score(X_test,y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
